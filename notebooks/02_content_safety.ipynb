{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 02. Content Safety Lab\n",
                "\n",
                "In this notebook, we evaluate our toxicity detection mechanisms against a dataset of toxic and clean comments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import json\n",
                "\n",
                "# Add project root to path\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
                "\n",
                "from intermediate.toxic_content_detection import ToxicDetector, ToxicResult"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Datasets\n",
                "We load both toxic and clean samples to check for false positives and negatives."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    with open('../data/samples/toxic_comments.json', 'r') as f:\n",
                "        toxic_samples = json.load(f)\n",
                "    with open('../data/samples/clean_queries.json', 'r') as f:\n",
                "        clean_samples = json.load(f)\n",
                "except FileNotFoundError:\n",
                "    print(\"Data not found. Using fallbacks.\")\n",
                "    toxic_samples = [\"You are stupid\"]\n",
                "    clean_samples = [\"Hello world\"]\n",
                "\n",
                "print(f\"Loaded {len(toxic_samples)} toxic and {len(clean_samples)} clean samples.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Evaluate Detector\n",
                "We run the detector on both sets and calculate accuracy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "detector = ToxicDetector()\n",
                "\n",
                "def evaluate(samples, expected_label):\n",
                "    correct = 0\n",
                "    for s in samples:\n",
                "        result = detector.check(s)\n",
                "        label = \"toxic\" if result == ToxicResult.TOXIC else \"clean\"\n",
                "        if label == expected_label:\n",
                "            correct += 1\n",
                "        else:\n",
                "            print(f\"Missed: '{s}' (Predicted: {label})\")\n",
                "    return correct / len(samples)\n",
                "\n",
                "print(\"--- Evaluating Toxic Samples ---\")\n",
                "toxic_acc = evaluate(toxic_samples, \"toxic\")\n",
                "print(f\"Accuracy on Toxic Data: {toxic_acc:.2%}\\n\")\n",
                "\n",
                "print(\"--- Evaluating Clean Samples ---\")\n",
                "clean_acc = evaluate(clean_samples, \"clean\")\n",
                "print(f\"Accuracy on Clean Data: {clean_acc:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Extending the Blocklist\n",
                "If we missed any, we can easily extend the detector."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "custom_patterns = [r\"\\bidiot\\b\"] # Example of adding a new pattern\n",
                "enhanced_detector = ToxicDetector(custom_patterns=custom_patterns)\n",
                "\n",
                "test_phrase = \"You are an idiot.\"\n",
                "print(f\"Standard Detector: {detector.check(test_phrase).value}\")\n",
                "print(f\"Enhanced Detector: {enhanced_detector.check(test_phrase).value}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}